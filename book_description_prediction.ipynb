{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Program Files\\miniconda3\\envs\\project\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(123)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_path = 'fine_tuned_models_gpt2\\\\model'\n",
    "\n",
    "tokenizer_path = 'fine_tuned_models_gpt2\\\\tokenizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Art-Photography': 0, 'Biography': 1, 'Business-Finance-Law': 2, 'Childrens-Books': 3, 'Computing': 4, 'Crafts-Hobbies': 5, 'Crime-Thriller': 6, 'Dictionaries-Languages': 7, 'Entertainment': 8, 'Food-Drink': 9, 'Graphic-Novels-Anime-Manga': 10, 'Health': 11, 'History-Archaeology': 12, 'Home-Garden': 13, 'Humour': 14, 'Medical': 15, 'Mind-Body-Spirit': 16, 'Natural-History': 17, 'Personal-Development': 18, 'Poetry-Drama': 19, 'Reference': 20, 'Religion': 21, 'Romance': 22, 'Science-Fiction-Fantasy-Horror': 23, 'Science-Geography': 24, 'Society-Social-Sciences': 25, 'Sport': 26, 'Stationery': 27, 'Teaching-Resources-Education': 28, 'Technology-Engineering': 29, 'Teen-Young-Adult': 30, 'Transport': 31, 'Travel-Holiday-Guides': 32}\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"book-covers\"\n",
    "\n",
    "CATEGORIES = next(os.walk(DATA_DIR), (None, None, []))[1]\n",
    "# print(CATEGORIES)\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {CATEGORIES[i]:i for i in range(len(CATEGORIES))}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)\n",
    "print(labels_ids)\n",
    "print(n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDescriptionDataset(Dataset):\n",
    "    def __init__(self, path, use_tokenizer):\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError('Invalid path variable.')\n",
    "        \n",
    "        book_info = pd.read_csv(path)\n",
    "        self.texts = book_info['text'].values\n",
    "        self.labels = book_info['label'].values\n",
    "        self.n_examples = len(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {'text': self.texts[item],\n",
    "                'label': self.labels[item]}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2ClassificationCollator(object):\n",
    "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
    "\n",
    "        # Tokenizer to be used inside the class.\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length.\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        # Label encoder used inside the class.\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        # Get all texts from sequences list.\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        # Get all labels from sequences list.\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        # Encode all labels using label encoder.\n",
    "        labels = [self.labels_encoder[label] for label in labels]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
    "        # appropriate padding.\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        # Update the inputs with the associated encoded labels as tensor.\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, device_):\n",
    "  # Use global variable for model.\n",
    "  global model\n",
    "\n",
    "  # Tracking variables\n",
    "  predictions_labels = []\n",
    "  true_labels = []\n",
    "  #total loss for this epoch.\n",
    "  total_loss = 0\n",
    "\n",
    "  # Put the model in evaluation mode--the dropout layers behave differently\n",
    "  # during evaluation.\n",
    "  model.eval()\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "    # add original labels\n",
    "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "    # move batch to device\n",
    "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and\n",
    "    # speeding up validation\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple along with the logits. We will use logits\n",
    "        # later to to calculate training accuracy.\n",
    "        loss, logits = outputs[:2]\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # get predicitons to list\n",
    "        predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "        # update list\n",
    "        predictions_labels += predict_content\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "  # Return all true labels and prediciton for future evaluations.\n",
    "  return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuraiton...\n",
      "Loading tokenizer...\n",
      "Loading model...\n",
      "Model loaded to `cuda`\n"
     ]
    }
   ],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_path, num_labels=n_labels)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=tokenizer_path)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_path, config=model_config)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with Test...\n",
      "Created `test_dataset` with 264 examples!\n",
      "Created `eval_dataloader` with 33 batches!\n"
     ]
    }
   ],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "print('Dealing with Test...')\n",
    "# Create pytorch dataset.\n",
    "test_dataset =  BookDescriptionDataset(path=os.path.join('book_cover_temp\\\\test_data_sorted.csv'), \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `test_dataset` with %d examples!'%len(test_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e6cf11265f4721801b3064cc2269c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_labels, predictions_labels, avg_epoch_loss = validation(test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "i = 0\n",
    "for t, p in zip(true_labels, predictions_labels):\n",
    "    if t == p:\n",
    "        indices.append(i)\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"prediction_output\\\\gpt2_prediction_output.txt\", np.array(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the Prediction Result Intersection from both models, save the index for the book recommendation website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "description_index = np.loadtxt(\"prediction_output\\\\gpt2_prediction_output.txt\")\n",
    "book_cover_index = np.loadtxt(\"prediction_output\\\\vit_prediction_output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   6.,  23.,  24.,  25.,  26.,  33.,  35.,  36.,  37.,\n",
       "        39.,  44.,  46.,  47.,  49.,  50.,  53.,  54.,  56.,  58.,  59.,\n",
       "        61.,  62.,  63.,  68.,  72.,  76.,  78.,  79.,  81.,  83.,  92.,\n",
       "        98., 100., 102., 103., 104., 106., 108., 110., 114., 118., 120.,\n",
       "       124., 126., 129., 130., 132., 135., 136., 141., 142., 149., 150.,\n",
       "       152., 153., 155., 156., 167., 168., 170., 172., 174., 176., 181.,\n",
       "       182., 184., 188., 190., 213., 218., 220., 221., 222., 226., 230.,\n",
       "       231., 232., 233., 234., 235., 238., 239., 240., 241., 242., 246.,\n",
       "       247., 250., 251., 252., 254., 256., 257., 258., 259., 262.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.,  32.,  34.,  40.,  43.,  48.,  54.,  56.,  70.,  71.,  72.,\n",
       "        76.,  79.,  80.,  92., 110., 115., 121., 125., 134., 135., 136.,\n",
       "       143., 145., 148., 158., 164., 171., 174., 178., 180., 182., 184.,\n",
       "       185., 189., 210., 212., 213., 215., 218., 238., 241., 243., 244.,\n",
       "       245., 246., 247., 249., 251., 253.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_cover_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = np.intersect1d(description_index, book_cover_index).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = [int(i) for i in intersection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54,\n",
       " 56,\n",
       " 72,\n",
       " 76,\n",
       " 79,\n",
       " 92,\n",
       " 110,\n",
       " 135,\n",
       " 136,\n",
       " 174,\n",
       " 182,\n",
       " 184,\n",
       " 213,\n",
       " 218,\n",
       " 238,\n",
       " 241,\n",
       " 246,\n",
       " 247,\n",
       " 251]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv('book_cover_temp\\\\temp_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_book_info = test_data.iloc[intersection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_book_info = selected_book_info[['volumeInfo.title', 'volumeInfo.description', 'volumeInfo.imageLinks.thumbnail', 'search_term']]\n",
    "selected_book_info = selected_book_info.rename(columns={'volumeInfo.title':'title', 'volumeInfo.description':'description', 'volumeInfo.imageLinks.thumbnail': 'thumbnail'})\n",
    "# selected_book_info['thumbnail'] = '<img src=\\\"' + selected_book_info['thumbnail'] + '\\\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_book_info = selected_book_info.fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Crime, Histoire &amp; Sociétés</td>\n",
       "      <td></td>\n",
       "      <td>http://books.google.com/books/content?id=xwttA...</td>\n",
       "      <td>Crime-Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Blue Moon</td>\n",
       "      <td>Reacher is on a Greyhound bus, minding his own...</td>\n",
       "      <td>http://books.google.com/books/content?id=V1zaD...</td>\n",
       "      <td>Crime-Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Drink Progressively</td>\n",
       "      <td>DRINK PROGRESSIVELY offers readers an easy and...</td>\n",
       "      <td>http://books.google.com/books/content?id=0Zo2M...</td>\n",
       "      <td>Food-Drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Food and Drink in American History</td>\n",
       "      <td>This three-volume encyclopedia on the history ...</td>\n",
       "      <td>http://books.google.com/books/content?id=o7gxB...</td>\n",
       "      <td>Food-Drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>We Shall Eat and Drink Again</td>\n",
       "      <td></td>\n",
       "      <td>http://books.google.com/books/content?id=kAMMA...</td>\n",
       "      <td>Food-Drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Archaeology For Dummies</td>\n",
       "      <td>An objective guide to this fascinating science...</td>\n",
       "      <td>http://books.google.com/books/content?id=GRz0y...</td>\n",
       "      <td>History-Archaeology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>The Sweet Life</td>\n",
       "      <td>An anthology of Laura Stoddart's exquisite, ti...</td>\n",
       "      <td>http://books.google.com/books/content?id=lvhlT...</td>\n",
       "      <td>Home-Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Mind and the New Physics</td>\n",
       "      <td></td>\n",
       "      <td>http://books.google.com/books/content?id=9VkdA...</td>\n",
       "      <td>Mind-Body-Spirit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>The Necessity of the Mind</td>\n",
       "      <td></td>\n",
       "      <td>http://books.google.com/books/content?id=b4pcA...</td>\n",
       "      <td>Mind-Body-Spirit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Origen's Contra Celsum</td>\n",
       "      <td>Presented here for the first time in years is ...</td>\n",
       "      <td>http://books.google.com/books/content?id=8Tz7D...</td>\n",
       "      <td>Religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Simple Gifts</td>\n",
       "      <td>Four holiday tales capture the magic and roman...</td>\n",
       "      <td>http://books.google.com/books/content?id=_y8kw...</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Passion's Promise</td>\n",
       "      <td>Kezia Saint Martin was born to dazzle the worl...</td>\n",
       "      <td>http://books.google.com/books/content?id=CtBvD...</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Understanding Deviance</td>\n",
       "      <td>In Understanding Deviance, Seventh Edition, le...</td>\n",
       "      <td>http://books.google.com/books/content?id=oCD4C...</td>\n",
       "      <td>Society-Social-Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Goal!</td>\n",
       "      <td>Santiago, a young Mexican American, fulfills h...</td>\n",
       "      <td>http://books.google.com/books/content?id=vso6v...</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Standard Handbook of Engineering Calculations</td>\n",
       "      <td>Now substantially revised and improved, this i...</td>\n",
       "      <td>http://books.google.com/books/content?id=liiv2...</td>\n",
       "      <td>Technology-Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Industry Integrated Engineering and Computing ...</td>\n",
       "      <td>This book introduces recent global advances an...</td>\n",
       "      <td>http://books.google.com/books/content?id=OUefD...</td>\n",
       "      <td>Technology-Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>The Little Train</td>\n",
       "      <td>In a new board book edition of the delightful ...</td>\n",
       "      <td>http://books.google.com/books/content?id=zgwGA...</td>\n",
       "      <td>Transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Carving the Western Path</td>\n",
       "      <td>R.G. (Bob) Harvey tells the stories of the roa...</td>\n",
       "      <td>http://books.google.com/books/content?id=7pyaJ...</td>\n",
       "      <td>Transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>The Trains Long Departed</td>\n",
       "      <td>The Trains Long Departed tells the story of Ir...</td>\n",
       "      <td>http://books.google.com/books/content?id=Uvmgc...</td>\n",
       "      <td>Transport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "54                          Crime, Histoire & Sociétés   \n",
       "56                                           Blue Moon   \n",
       "72                                 Drink Progressively   \n",
       "76                  Food and Drink in American History   \n",
       "79                        We Shall Eat and Drink Again   \n",
       "92                             Archaeology For Dummies   \n",
       "110                                     The Sweet Life   \n",
       "135                           Mind and the New Physics   \n",
       "136                          The Necessity of the Mind   \n",
       "174                             Origen's Contra Celsum   \n",
       "182                                       Simple Gifts   \n",
       "184                                  Passion's Promise   \n",
       "213                             Understanding Deviance   \n",
       "218                                              Goal!   \n",
       "238      Standard Handbook of Engineering Calculations   \n",
       "241  Industry Integrated Engineering and Computing ...   \n",
       "246                                   The Little Train   \n",
       "247                           Carving the Western Path   \n",
       "251                           The Trains Long Departed   \n",
       "\n",
       "                                           description  \\\n",
       "54                                                       \n",
       "56   Reacher is on a Greyhound bus, minding his own...   \n",
       "72   DRINK PROGRESSIVELY offers readers an easy and...   \n",
       "76   This three-volume encyclopedia on the history ...   \n",
       "79                                                       \n",
       "92   An objective guide to this fascinating science...   \n",
       "110  An anthology of Laura Stoddart's exquisite, ti...   \n",
       "135                                                      \n",
       "136                                                      \n",
       "174  Presented here for the first time in years is ...   \n",
       "182  Four holiday tales capture the magic and roman...   \n",
       "184  Kezia Saint Martin was born to dazzle the worl...   \n",
       "213  In Understanding Deviance, Seventh Edition, le...   \n",
       "218  Santiago, a young Mexican American, fulfills h...   \n",
       "238  Now substantially revised and improved, this i...   \n",
       "241  This book introduces recent global advances an...   \n",
       "246  In a new board book edition of the delightful ...   \n",
       "247  R.G. (Bob) Harvey tells the stories of the roa...   \n",
       "251  The Trains Long Departed tells the story of Ir...   \n",
       "\n",
       "                                             thumbnail  \\\n",
       "54   http://books.google.com/books/content?id=xwttA...   \n",
       "56   http://books.google.com/books/content?id=V1zaD...   \n",
       "72   http://books.google.com/books/content?id=0Zo2M...   \n",
       "76   http://books.google.com/books/content?id=o7gxB...   \n",
       "79   http://books.google.com/books/content?id=kAMMA...   \n",
       "92   http://books.google.com/books/content?id=GRz0y...   \n",
       "110  http://books.google.com/books/content?id=lvhlT...   \n",
       "135  http://books.google.com/books/content?id=9VkdA...   \n",
       "136  http://books.google.com/books/content?id=b4pcA...   \n",
       "174  http://books.google.com/books/content?id=8Tz7D...   \n",
       "182  http://books.google.com/books/content?id=_y8kw...   \n",
       "184  http://books.google.com/books/content?id=CtBvD...   \n",
       "213  http://books.google.com/books/content?id=oCD4C...   \n",
       "218  http://books.google.com/books/content?id=vso6v...   \n",
       "238  http://books.google.com/books/content?id=liiv2...   \n",
       "241  http://books.google.com/books/content?id=OUefD...   \n",
       "246  http://books.google.com/books/content?id=zgwGA...   \n",
       "247  http://books.google.com/books/content?id=7pyaJ...   \n",
       "251  http://books.google.com/books/content?id=Uvmgc...   \n",
       "\n",
       "                 search_term  \n",
       "54            Crime-Thriller  \n",
       "56            Crime-Thriller  \n",
       "72                Food-Drink  \n",
       "76                Food-Drink  \n",
       "79                Food-Drink  \n",
       "92       History-Archaeology  \n",
       "110              Home-Garden  \n",
       "135         Mind-Body-Spirit  \n",
       "136         Mind-Body-Spirit  \n",
       "174                 Religion  \n",
       "182                  Romance  \n",
       "184                  Romance  \n",
       "213  Society-Social-Sciences  \n",
       "218                    Sport  \n",
       "238   Technology-Engineering  \n",
       "241   Technology-Engineering  \n",
       "246                Transport  \n",
       "247                Transport  \n",
       "251                Transport  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_book_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_book_info.to_csv('book_cover_recommendation_page\\\\documents\\\\selected_books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"book_recommendation.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(str(int(s)) for s in intersection))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "561fef862d7d0f6d08eb155e479d91bf2ec42edf385df79e76a15660ccc1c65c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
